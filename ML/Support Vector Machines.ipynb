{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b204eb5",
   "metadata": {},
   "source": [
    "# Support Vector Machine\n",
    "\n",
    "## Introduction and Formulation\n",
    "\n",
    "A Support Vector Machine (SVM) is a **supervised** learning model for **Classification and Regression** analysis. SVM maps training examples to points in space so as to maximize the width of the gap between the two categories. \n",
    "\n",
    "<figure>\n",
    "    <center><img src=\"img/SVM.png\" width=\"300\" height=\"300\">\n",
    "    <figcaption>Fig: Linear SVM</figcaption></center>\n",
    "</figure>\n",
    "\n",
    "Let $\\{x_i\\}_{i=1}^n$ the training set with classes $\\{ -1 , 1\\}$. The separation hyperplane is defined by \n",
    "$$ \\{ x \\in \\mathbb{R^n} | w^{\\top}x + b = 0 \\} $$\n",
    "\n",
    "So that if $w^{\\top}x + b > 0$ then $x$ belongs to the class 1 and if $w^{\\top}x + b < 0$ then $x$ belongs to the class -1. \n",
    "\n",
    "$w \\in \\mathbb{R}^n$ is the perpendicular vector to the hyperplane and $b \\in \\mathbb{R}$ the *offset*. When the data is linearly separable, there are infinite separation hyperplanes, so we want to impose the class membership constraint on the support vectors $x_{+} , x_{-}$\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "w^{\\top}x_{+} + b = 1 \\\\\n",
    "w^{\\top}x_{-} + b = -1\n",
    "\\end{split}\n",
    "\\end{equation} $$\n",
    "\n",
    "The width of the gap $m$ using the restriction above can be calculated by \n",
    "$$\n",
    "m = \\frac{1}{2}||proy_w(x_{+} - x_{-})|| = \\frac{1}{2}||x_{+} - x_{-}||\\cos(\\theta)\n",
    "$$\n",
    "Using that $cos(\\theta) = \\frac{\\langle x,y \\rangle}{||x|| \\cdot ||y||}$\n",
    "$$\n",
    "= \\frac{1}{2}||x_{+} - x_{-}||\\left ( \\frac{w^{\\top}(x_{+} - x_{-})}{||w|| \\cdot ||x_{+} - x_{-}||} \\right ) = \\frac{1}{2||w||}w^{\\top}(x_{+} - x_{-}) = \\frac{1}{||w||}$$\n",
    "As the support vectors are the closest points to the hyperplane that satisfy the constraint with equality, then the classification rule can be written as \n",
    "\n",
    "$$\n",
    "\\begin{equation*}\n",
    "\\begin{split}\n",
    "y_i = +1 \\leftrightarrow w^{\\top}x_i + b \\geq +1 \\\\\n",
    "y_i = -1 \\leftrightarrow w^{\\top}x_i + b \\leq -1\n",
    "\\end{split}\n",
    "\\end{equation*} $$\n",
    "\n",
    "And the optimization problem is given by\n",
    "$$\n",
    "\\begin{equation*}\n",
    "\\begin{aligned}\n",
    "\\max_{\\omega , b} \\quad \\frac{1}{||w||} \\\\\n",
    "\\textrm{s.t.} \\quad y_i(w^{\\top}x_i + b ) \\geq 1 , \\forall i \\in \\{ 1 , \\dots N \\}\n",
    "\\end{aligned}\n",
    "\\end{equation*}\n",
    "$$\n",
    "equivalent to \n",
    "$$\n",
    "\\begin{equation*}\n",
    "\\begin{aligned}\n",
    "\\min_{\\omega , b} \\quad \\frac{1}{2}||w||^2 \\\\\n",
    "\\textrm{s.t.} \\quad y_i(w^{\\top}x_i + b ) \\geq 1 , \\forall i \\in \\{ 1 , \\dots N \\}\n",
    "\\end{aligned}\n",
    "\\end{equation*}\n",
    "$$\n",
    "This optimization problem is often solved using the **dual problem** (quadratic programming) and is fundamental for the non-linear extension of the SVM. (Kernel Trick)\n",
    "\n",
    "## Soft Margin\n",
    "\n",
    "The data is often not linearly separable so we have to allow the missclassification of some points (**Soft Margin**). This is made by changing the optimization problem (adding regularization) to \n",
    "$$\n",
    "\\begin{equation*}\n",
    "\\begin{aligned}\n",
    "\\min_{\\omega , b} \\quad \\frac{1}{2}||w||^2 + C \\sum_{i=1}^N \\xi_i  \\\\\n",
    "\\textrm{s.t.} \\quad y_i(w^{\\top}x_i + b ) \\geq 1  - \\xi_i, \\forall i \\in \\{ 1 , \\dots N \\} \\hspace{0.1cm} ,\\hspace{0.1cm} \\xi_i \\geq 0\n",
    "\\end{aligned}\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "## Kernel Trick\n",
    "\n",
    "In the formulation of the dual problem, the objective function requires computing the dot products between all the data points on the training set. If we want to project our data to a higher dimension (apply a mapping $\\phi$ and with that, make the separation possible), this can be done just computing the products $ \\langle <\\phi(x_i) , \\phi(x_j)> \\rangle $ for all $i$ and $j$. \n",
    "\n",
    "<figure>\n",
    "    <center><img src=\"img/mapping.png\" width=\"500\" height=\"300\">\n",
    "    <figcaption>Fig: Mapping to a Higher Dimension</figcaption></center>\n",
    "</figure>\n",
    "\n",
    "\n",
    "The benefit of this is that we don't need to know the mapping $\\phi$ as **Mercer Theorem** states that \n",
    "$$\n",
    "K(x_1 , x_2) = <\\phi(x_i) , \\phi(x_j)>\n",
    "$$\n",
    "where $K: X \\times X \\rightarrow \\mathbb{R} $ is a Mercer Kernel in a Hilbert Space (possibly of infinite dimension) so we have to set $K$ in order to map the data to a higher dimension.\n",
    "\n",
    "## Important Parameters\n",
    "\n",
    " - C: Is the regularization constant. Default = 1 \n",
    " - Kernel: The kernel used to compute the dot products. Default = \"RBF\" (Infinite Dimensional) , others: Poly (Finite), Sigmoid (Infinite), Linear (Finite)\n",
    " - Kernel parameters: Degree, Gamma/Scale, etc... \n",
    "\n",
    "## Relevant Information \n",
    " - SVD are **sensitive to feature scaling**\n",
    " - Approx Complexity: $O(M \\cdot N)$ linear case and $O(M^2 \\cdot N)$ to $O(M^3 \\cdot N)$ when using Kernel trick. $N$ number of attributes and $M$ the number of instances. \n",
    "\n",
    "\n",
    "## Implementation\n",
    "\n",
    "\n",
    "We are going to work with the [Digits](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html) Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "672b79a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import usual libraries\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "#Import usual functions\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report , accuracy_score\n",
    "\n",
    "#Import utils\n",
    "from utils.plot import confusion_matrix_custom\n",
    "\n",
    "#Import required libraries and functions\n",
    "from sklearn.datasets import load_iris, load_digits\n",
    "from sklearn.tree import DecisionTreeClassifier\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "86ced299d573a58fc1d464bd2f917835a691b9d9cefee14c59fce60008360703"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
