\section{Others}

\subsection{Performance Metrics} 

Para medir el performance de un modelo de \textit{Machine Learning} ya sea en problemas de clasificación o regresión, es posible utilizar distintas métricas de acuerdo a lo que nos importa medir en cada situación. 

\subsubsection{Classification Metrics}

\begin{figure}[H]
    \center
    \includegraphics[scale=0.25]{notebooks/Others/img/confusion_matrix_diagram.png}
    \caption{Confusion Matrix Diagram}
\end{figure}

En las métricas para problemas de clasificación, podemos encontrar
\begin{enumerate}
    \item \textbf{Accuracy:} Se define como el total de aciertos positivos y negativos sobre el total de predicciones. 
    $$ 
    \text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN}
    $$
    \item \textbf{Precision:} Este es el porcentaje de identificaciones positivas correctas en el total de identificaciones positivas. 
    $$
    \text{Precision} = \frac{TP}{TP + FP}
    $$
    \item \textbf{Recall: } Este es el porcentaje de identificaciones positivas correctas en el total de datos positivos. 
    $$ 
    \text{Recall} = \frac{TP}{TP + FN}
    $$
    \item \textbf{F1 - Score: } Esta métrica es la media armónica entre la precisión y el recall. 
    $$ 
    F_{1} = \frac{2}{\frac{1}{\text{precision}} + \frac{1}{\text{recall}}} = 2 \frac{\text{precision} \cdot \text{recall}}{\text{precision} + \text{recall}}
    $$
    Si alguna de las métricas tiene una mayor importancia dado el contexto del problema, podemos amplificarla por un valor $\beta$ de la siguiente forma
    $$ 
    F_{\beta} = (1 + \beta^2) \frac{\text{precision} \cdot \text{recall}}{\beta^2 \cdot \text{precision} + \text{recall}}
    $$
    Es decir, el recall es $\beta$ veces más importante que la precisión. 
\end{enumerate}

En problemas de clasificación binario, definir si un score se asigna al label positivo (1) o negativo (0) depende de un umbral (threshold) que se puede definir en base a qué métrica queremos optimizar. Lo anterior da origen a la curva \textit{Precision - Recall}

\begin{figure}[H]
    \center
    \includegraphics[scale=0.5]{notebooks/Others/img/precision_recall_f1_curve.png}
    \caption{Precision-Recall-F1 Curve}
\end{figure}

Es fácil ver que el máximo de la métrica $F_{1}$ se alcanza en la intersección de la Precision y el Recall por la desigualdad AM-GM:
$$ 
\frac{n}{\frac{1}{x_1} + \dots + \frac{1}{x_n}} \leq \sqrt[n]{x_1 \dots x_n}
$$

Existen otras medidas que no dependen del threshold considerado, como: 

\begin{enumerate}
    \item \textbf{AUC: } Esta medida (\textit{Area Under the Curve)} se construye a partir de la tasa de TP y FP para todos los posibles umbrales de clasificación. 

    \begin{figure}[H]
    \center
    \includegraphics[scale=0.1]{notebooks/Others/img/roc_curve_diagram.png}
    \caption{ROC Curve Diagram}
    \end{figure}

    El área por debajo de la curva ROC es lo que se conoce como AUC. Una mayor área indica un mejor rendimiento del modelo. 

    Una interpretación de esta medida, se puede entender cómo "La probabilidad de escoger el ejemplo con el label positivo dado que se presenta uno con label positivo y otro con label negativo", o bien, hablar del buen ordenamiento de los ejemplos positivos y los ejemplos negativos. 
    
    \item \textbf{Gini Index: } El índice de Gini se construye de manera similar al AUC, la conversión es directa según 
    $$ 
    \text{Gini} = 2 \cdot \text{AUC} - 1 
    $$
    \begin{figure}[H]
    \center
    \includegraphics[scale=0.3]{notebooks/Others/img/gini_diagram.png}
    \caption{Gini Diagram}
    \end{figure}
    
\end{enumerate}

\subsubsection{Regression Metrics}

Para problemas de regresión, es posible utilizar las siguientes métricas 
\begin{enumerate}
    \item \textbf{MSE}: El error cuadrático medio definido como 
    $$ 
    \text{MSE} = \sqrt{\frac{1}{N}\sum_{i=1}^N(y_i - \hat{y}_i)^2}
    $$
    \item \textbf{MAE}: El error absoluto medio definido como 
    $$ 
    \text{MAE} = \frac{1}{N}\sum_{i=1}^N|y_i - \hat{y}_i|
    $$
    \item \textbf{R Squared}: El coeficiente de determinación $R^2$ es ampliamente utilizado para medir el poder predictivo de una regresión lineal. Es un valor que oscila entre 0 y 1 y está definido como 
    $$ 
    R^2 = 1 - \frac{SS_{\text{res}}}{SS_{\text{tot}}}
    $$
    Donde $SS_{\text{res}}$ es la suma de los cuadrados de las diferencias entre el valor real y la predicción. $SS_{\text{tot}}$ es la suma de los cuadrados de las diferencias entre el valor real y el valor medio de la variable (similar al cálculo de varianza). 

    \begin{figure}[H]
    \center
    \includegraphics[scale=0.1]{notebooks/Others/img/r_squared_diagram.png}
    \caption{R Squared Diagram}
    \end{figure}

    Existe una variación \textbf{Adjusted R Squared} que toma en consideración además la cantidad $M$ de features en el modelo. 

    $$
    R^2_{\text{Adjusted}} = 1 - \frac{(1-R^2)(N-1)}{N-M-1}
    $$

    La idea es controlar el overfitting al agregar más variables al modelo mejorando el $R^2$ pero no así el $R^2_{\text{Adjusted}}$.
    
\end{enumerate}

\subsection{Bias vs Variance}

El dilema de \textit{Bias vs Variance} describe la relación entre la complejidad del modelo, la precisión de las predicciones y cómo éste se comporta al predecir datos nunca antes vistos. El error estimado de una predicción viene dado en términos generales por 
$$
\text{Expected Error} = (\text{Bias})^2 + \text{Variance} + \text{Irreductible Error}
$$
Así, un modelo que crece en complejidad reducirá su bias pero aumentará su varianza (extremo: overfitting) y a la vez, reducir la complejidad permitirá generalizar mejor reduciendo la varianza pero aumentando el bias (extremo: underfitting). 

\begin{figure}[H]
    \center
    \includegraphics[scale=0.25]{notebooks/Others/img/bias_vs_variance.png}
    \caption{Bias vs Variance Diagram}
\end{figure}

\subsection{Oversampling and Undersampling}

\subsection{Random Noise Feature Importance}

\subsection{SHAP Values}
\label{subsec:shap_values}

El SHAP values (\textit{SHapley Additive exPlanations}) es un algoritmo modelo-agnóstico basado en teoría de juegos que permite \textbf{interpretar las predicciones}, la importancia de las variables y su impacto.

\begin{figure}[H]
    \center
    \includegraphics[scale=0.55]{notebooks/Others/img/shap_values_example.png}
    \caption{SHAP Values Example}
\end{figure}

\subsection{Outlier Detection}

La detección de outliers es la práctica de encontrar \textbf{datos anómalos} o fuera de distribución en un dataset. Es de suma importancia para la construcción de modelos de fraude o para mejorar el performance de modelos sensibles a outliers.

\begin{figure}[H]
    \center
    \includegraphics[scale=0.55]{notebooks/Others/img/outlier_detection.png}
    \caption{Outiler Detection Contour Plot}
\end{figure}

\subsubsection{Interquantile Range}

Definimos el \textit{Interquantile Range} IC como la diferencia entre el percentil 75 y el percentil 25. Es decir, 
$$ 
IC = Q_3 - Q_1
$$
En este caso, los outliers serán aquellos datos $x_i$ tal que 
$x_i \geq Q3 + I_{\text{range}}IC$ o bien $x_i \leq Q1 - I_{\text{range}}IC$
donde $ I_{\text{range}}$ permite controlar el porcentaje de outliers que se espera encontrar. (Equivalencia en distribución normal $I_{\text{range}} = 1.5 \approx Z_{\text{value}} = 2.69$).

Notar que en esta metodología no se asume normalidad en la distribución de los datos. 

\subsubsection{Z - Score}

Cuando los datos siguen una distribución normal, se pueden considerar como outliers aquellos datos $x_i$ que cumplen 
$$\left | \frac{x_i - \mu}{\sigma}\right | \geq Z_{\text{value}}$$

Para los casos con más de una feature (multidimensional), se puede extender esta definición utilizando la \textbf{distancia de Mahalanobis} definida como 
$$
D = \sqrt{(x_i - \mu_i)^{\top}\Sigma^{-1}(x_i - \mu_i)}
$$
Donde $\Sigma$ es la matriz de covarianza y $\mu_i$ el vector de media. 

\subsubsection{DBSCAN}

Los datos que no cumplan el mínimo de vecinos a distancia menor de $\epsilon$ son considerados por DBSCAN como outliers.

\subsubsection{Isolation Forest}

El algoritmo de \textit{Isolation Forest} selecciona de manera aleatoria una feature y un corte aleatorio entre el mínimo y el máximo valor presente en esa feature. Esto es repetido múltiples veces hasta aislar cada uno de los puntos.

Este algoritmo iterativo, puede ser representado en el siguiente diagrama:

\begin{figure}[H]
    \center
    \includegraphics[scale=0.3]{notebooks/Others/img/isolation_forest_diagram.png}
    \caption{Isolation Forest Diagram}
\end{figure}

Los puntos que en promedio, requieren \textbf{menos divisiones para ser aislados}, son los que el algoritmo considera como potenciales datos anómalos (outliers).

\subsection{Cross-Validation Techniques}

El \textit{Cross-Validation} es un método que permite evaluar el rendimiento de un modelo de \textit{Machine Learning} y que busca eliminar el sesgo de la elección del conjunto de entrenamiento y testeo. Es ampliamente utilizado para la búsqueda de los mejores hiper-parámetros de un modelo. 


\begin{enumerate}
    \item \textbf{KFold}: Esta técnica consiste en dividir el conjunto de entrenamiento en $k$ partes. En cada iteración, se selecciona una de las particiones para testing y el resto para training. El score será el promedio de los scores obtenidos. Existe una variación llamada \textbf{StratifiedKFold} en la cual se asegura además que el target tenga la misma distribución en cada partición.
    \item \textbf{ShuffleSplit}: Esta técnica selecciona la partición de entrenamiento y testing de manera aleatoria. No asegura que las particiones sean las mismas en cada iteración. 
    \item \textbf{LeavePOut}: Esta técnica deja $p$ datos para el conjunto de testeo y entrena con todo el resto. Este proceso se realiza con todas las combinaciones posibles lo que asegura una estimación con menor bias aunque es extremadamente ineficiente. 
\end{enumerate}

\subsubsection{Time Series Cross Validation}

En el caso de series de tiempo, es importante notar que al momento de realizar \textit{Cross-Validation} para problemas de Forecast, es importante \textbf{no entrenar con datos posteriores al testing} pues en la práctica, no se tendría acceso a estos.

\begin{figure}[H]
    \center
    \includegraphics[scale=0.45]{notebooks/Others/img/time_series_cross_validation.png}
    \caption{Time Series Cross-Validation Example}
\end{figure}








