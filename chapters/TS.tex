\chapter{Time Series}

\section{Fourier Series}

La serie de Fourier es un proceso analítico que permite representar una función $f(t)$ en base a funciones sinusoidales, es decir, 

$$
f(t) = \frac{a_0}{2} + \sum_{n=1}^{\infty} (a_n \cos(nt) + b_n \sin(nt))
$$
y el objetivo es determinar los coeficientes $a_n$ y $b_n$ que hacen esto posible. Se puede mostrar que
\begin{equation*}
\begin{aligned}
a_0 &= \frac{1}{\pi}\int_{-\pi}^{\pi} f(t)dt \\
a_n &= \frac{1}{\pi}\int_{-\pi}^{\pi} f(t)cos(nt)dt \quad \forall n > 0 \\ 
b_n &= \frac{1}{\pi}\int_{-\pi}^{\pi} f(t)sin(nt)dt \quad \forall n > 0
\end{aligned}
\end{equation*}

También es posible reescribir esta ecuación mediante notación compleja según 
$$ 
f(t) = \sum_{-\infty}^{\infty} c_n e^{int}
$$
Notando que 
$$ 
\cos(nt) = \frac{e^{int} + e^{-int}}{2} \quad y \quad \sin(nt) = \frac{e^{int} - e^{-int}}{2i} 
$$
Donde los coeficientes $c_n$ se calculan según 
$$
c_n = \frac{1}{2\pi}\int_{-\pi}^{\pi}f(t)e^{-int}dt
$$

Esta descomposición es especialmente útil para suavizar funciones al truncar la cantidad de términos sinusoidales de su serie de Fourier. 

\begin{figure}[H]
    \center
    \includegraphics[scale=0.5]{notebooks/TS/img/fourier_transform_smoothing.png}
    \caption{Fourier Series Smoothing}
\end{figure}

\section{Exponential Smoothing}

Esta método, que permite en principio suavizar series de tiempo, también puede ser utilizado para realizar el forecasting de un paso al futuro. La formulación es simple, sea $\{x_t\}$ la serie de tiempo, se define el paso $s_t$ según 
$$ 
s_t = \alpha x_t + (1 - \alpha) s_{t-1} \quad t > 0 
$$
Donde $s_0 = x_0$ y $\alpha$ el parámetro de suavizado. Valores cercanos a 1 dan más peso a las observaciones actuales y valores cercanos a 0 aumentan el suavizado de la función. Es importante notar que para forecasting $s_{t+1} = s_{t}$ y también los siguientes pasos futuros serán todos constantes. 



\section{ARIMA}

ARIMA es la abreviación de \textit{Auto-Regressive Integrated Moving Average} y es un método estadístico para realizar forecasting sobre series de tiempo que integra los siguientes conceptos:

\begin{enumerate}
    \item Toma en consideración patrones de crecimiento/decrecimiento en la serie de tiempo (\textit{Auto-Regressive}). 
    \item Estima tasa de crecimiento/decrecimiento (\textit{Integrated}).
    \item Controla el ruido entre datos consecutivos en el tiempo (\textit{Moving Average}).
\end{enumerate}

La fórmula general para este tipo de modelos viene dada por 
$$
Y_t = c + \phi_1y^d_{t-1} + \dots + \phi_py^d_{t-p} + \theta_1e_{t-1} + \dots + \theta_q e_{t-q} + e_t
$$
Aquí $c$ es una constante y $e$ es un término de error. Los modelos de este tipo son escritos como ARIMA($p,d,q$) donde: 

\begin{itemize}
    \item $p$ es la cantidad de tiempos en que la variable es mirada al pasado (Lag).
    \item $d$ es la cantidad de veces que la variable es diferenciada para producir una señal estacionaria. $d=0$ refiere a que la señal ya es estacionaria, $d=1$ es que la señal crece/decrece linealmente y $d=2$ es que la señal crece exponencialmente. 
    \item $q$ representa la cantidad de lag para el término de error $e$, esto captura el \textit{Moving Average}.
\end{itemize}

\subsection{p Value}

En la práctica, es posible determinar el valor de $p$ a través del \textit{Partial Autocorrelation Plot}. Este gráfico muestra la relación de un valor en la serie de tiempo con \textbf{un solo lag} (eliminando relaciones de tiempos intermedios ajustando una regresión lineal y quedándose sólo con el parámetro correspondiente).
\begin{figure}[H]
    \center
    \includegraphics[scale=0.5]{notebooks/TS/img/partial_autocorrelation.png}
    \caption{Partial Autocorrelation Plot}
\end{figure}
El valor óptimo es el último punto después del cual todos los lags están dentro de las bandas azules (intervalos de confianza), en este caso $p=13$. 

\subsection{d Value}

El valor de $d$ se puede calcular diferenciando la serie de tiempo hasta encontrar una serie estacionaria. Esto lo podemos medir con un test estadístico de estacionalidad (\textit{Augmented Dickey Fuller Test}).

\begin{figure}[H]
    \center
    \includegraphics[scale=0.5]{notebooks/TS/img/time_series_differentiation.png}
    \caption{Time Series Differentiation Plot}
\end{figure}

Vemos que el $p$-valor en la segunda diferenciación ya es lo suficientemente pequeño para asumir estacionalidad en la serie de tiempo, así $d=2$. 

\subsection{q Value}
Finalmente, para determinar el valor $q$ es posible ver el \textit{Autocorrelation Plot} que muestra la relación de un valor en la serie de tiempo con \textbf{todos los $p$ lags} anteriores. 
\begin{figure}[H]
    \center
    \includegraphics[scale=0.5]{notebooks/TS/img/autocorrelation.png}
    \caption{Autocorrelation Plot}
\end{figure}
En este caso, el último valor anterior a que todos los lag caigan en la zona azul, es $q=12$.
\begin{figure}[H]
    \center
    \includegraphics[scale=0.5]{notebooks/TS/img/arima_results.png}
    \caption{ARIMA over Flight Passengers Forecasting}
\end{figure}

\section{ML Forecasting}

Es posible utilizar algoritmos clásicos de \textit{Machine Learning} al adaptar la serie de tiempo a la creación de un dataset en base a \textit{lags}. 

\begin{figure}[H]
    \center
    \includegraphics[scale=0.15]{notebooks/TS/img/ml_forecast.png}
    \caption{Machine Learning Forecast Dataset}
\end{figure}

Este enfoque además permite la incorporación de \textbf{variables exógenas} que enriquecen la predicción del modelo.

El forecasting se realiza de manera iterativa, es decir, se utiliza la predicción anterior como input para la siguiente. Existe otra alternativa que acumula menor incertidumbre al entrenar un modelo distinto para predecir cada tiempo futuro.

\subsection{Residuals Bootstrapping}

Para estimar la incertidumbre de la predicción se puede asumir que errores futuros serán como los errores pasados. Sampleando de la colección de errores vistos en el pasado, es posible crear múltiples forecast de la siguiente forma 

\begin{figure}[H]
    \center
    \includegraphics[scale=0.16]{notebooks/TS/img/boostrapped_residuals.png}
    \caption{Bootstrapped Residuals}
\end{figure}

Con los $N$ forecast creados, es posible estimar el intervalo de confianza o los parámetros de alguna distribución que represente la incertidumbre.

\begin{figure}[H]
    \center
    \includegraphics[scale=0.6]{notebooks/TS/img/residuals.png}
    \caption{Residuals}
\end{figure}

\begin{figure}[H]
    \center
    \includegraphics[scale=0.5]{notebooks/TS/img/residual_bootstrapping_simulations.png}
    \caption{Bootstrapping Residual Simulations}
\end{figure}

Ahora bien, si los residuos siguen una distribución normal, el \textit{boostrapping} sera equivalente a samplear de una distribución normal centrada en 0 y de cierta varianza.

